# AWS S3 Connector

The AWS S3 connector provides Akka Stream sources and sinks to connect to [Amazon S3](https://aws.amazon.com/s3/).
S3 stands for Simple Storage Service and is an object storage service with a web service interface.


### Reported issues

[Tagged issues at Github](https://github.com/akka/alpakka/labels/p%3Aaws-s3)


## Artifacts

@@dependency [sbt,Maven,Gradle] {
  group=com.lightbend.akka
  artifact=akka-stream-alpakka-s3_$scalaBinaryVersion$
  version=$version$
}

## Usage

### Set up your S3 clients

The S3 connector can be configured within your `application.conf` file.

Configuration
: @@snip ($alpakka$/s3/src/main/resources/reference.conf)

### Create an S3 client

Scala
: @@snip ($alpakka$/s3/src/test/scala/akka/stream/alpakka/s3/scaladsl/S3SourceSpec.scala) { #client }

Java
: @@snip ($alpakka$/s3/src/test/java/akka/stream/alpakka/s3/javadsl/S3ClientTest.java) { #client }

### Storing a file in S3

Scala
: @@snip ($alpakka$/s3/src/test/scala/akka/stream/alpakka/s3/scaladsl/S3SinkSpec.scala) { #upload }

Java
: @@snip ($alpakka$/s3/src/test/java/akka/stream/alpakka/s3/javadsl/S3ClientTest.java) { #upload }

### Downloading a file from S3

Scala
: @@snip ($alpakka$/s3/src/test/scala/akka/stream/alpakka/s3/scaladsl/S3SourceSpec.scala) { #download }

Java
: @@snip ($alpakka$/s3/src/test/java/akka/stream/alpakka/s3/javadsl/S3ClientTest.java) { #download }

In order to download a range of a file's data you can use overloaded method which
additionally takes `ByteRange` as argument.

Scala
: @@snip ($alpakka$/s3/src/test/scala/akka/stream/alpakka/s3/scaladsl/S3SourceSpec.scala) { #rangedDownload }

Java
: @@snip ($alpakka$/s3/src/test/java/akka/stream/alpakka/s3/javadsl/S3ClientTest.java) { #rangedDownload }

#### Accessing object metadata

When downloading an object you also get the object's metadata with it. 
Here's an example of using this metadata to stream an object back to a client in akka http

```scala
val (data, eventualMeta) = s3Client.download(bucket, objectKey)
complete( eventualMeta.map ( meta â‡’
  HttpResponse(
    entity = HttpEntity(
      meta.contentType.flatMap(ContentType.parse(_).toOption).getOrElse(`application/octet-stream`),
      meta.contentLength,
      data
    )
  )
))
```

### List bucket contents

Scala
: @@snip ($alpakka$/s3/src/test/scala/akka/stream/alpakka/s3/scaladsl/S3SourceSpec.scala) { #list-bucket }

Java
: @@snip ($alpakka$/s3/src/test/java/akka/stream/alpakka/s3/javadsl/S3ClientTest.java) { #list-bucket }

### Copy upload (multi part)

Copy an S3 object from source bucket to target bucket using multi part copy upload.

Scala
: @@snip ($alpakka$/s3/src/test/scala/akka/stream/alpakka/s3/scaladsl/S3SinkSpec.scala) { #multipart-copy }

Scala
: @@snip ($alpakka$/s3/src/test/scala/akka/stream/alpakka/s3/scaladsl/S3SinkSpec.scala) { #multipart-copy-versioned }

Java
: @@snip ($alpakka$/s3/src/test/java/akka/stream/alpakka/s3/javadsl/S3ClientTest.java) { #multipart-copy }

### Java examples with custom headers

```java
import java.util.concurrent.CompletionStage;
import akka.actor.ActorSystem;
import akka.stream.ActorMaterializer;
import akka.stream.Materializer;
import akka.stream.alpakka.s3.MemoryBufferType;
import akka.stream.alpakka.s3.S3Settings;
import akka.stream.alpakka.s3.acl.CannedAcl;
import akka.stream.alpakka.s3.impl.ListBucketVersion2;
import akka.stream.alpakka.s3.impl.S3Headers;
import akka.stream.alpakka.s3.impl.ServerSideEncryption;
import akka.stream.alpakka.s3.javadsl.MultipartUploadResult;
import akka.stream.alpakka.s3.javadsl.S3Client;
import akka.testkit.javadsl.TestKit;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.AWSCredentialsProvider;
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.regions.AwsRegionProvider;

public class Test extends TestKit {

    public Test(ActorSystem system) {
        super(system);
    }

    private final Materializer materializer = ActorMaterializer.create(getSystem());

    //#client
    private final AWSCredentialsProvider credentials = new AWSStaticCredentialsProvider(
            new BasicAWSCredentials("my-AWS-access-key-ID", "my-AWS-password")
    );

    private AwsRegionProvider regionProvider(String region) {
        return new AwsRegionProvider() {
            @Override
            public String getRegion() throws SdkClientException {
                return region;
            }
        };
    }

    private final S3Settings settings = new S3Settings(
            MemoryBufferType.getInstance(),
            null,
            credentials,
            regionProvider("us-east-1"),
            false,
            scala.Option.empty(),
            ListBucketVersion2.getInstance()
    );

    private final S3Client client = new S3Client(settings, getSystem(), materializer);

    public CompletionStage<MultipartUploadResult> aes256Encryption(String sourceBucket, String sourceKey,
                                                                   String targetBucket, String targetKey) {
        return client.multipartCopy(sourceBucket, sourceKey, targetBucket, targetKey, S3Headers.empty(),
                ServerSideEncryption.AES256$.MODULE$);
    }

    public CompletionStage<MultipartUploadResult> cannedAcl(String sourceBucket, String sourceKey,
                                                            String targetBucket, String targetKey) {
        return client.multipartCopy(sourceBucket, sourceKey, targetBucket, targetKey,
                S3Headers.apply(CannedAcl.Private$.MODULE$), null);
    }
}
```

### Running the example code

The code in this guide is part of runnable tests of this project. You are welcome to edit the code and run it in sbt.

Scala
:   ```
    sbt
    > s3/test
    ```

Java
:   ```
    sbt
    > s3/test
    ```

# Bluemix Cloud Object Storage with S3 API

The Alpakka S3 connector can connect to a range of S3 compatible services. One of them is IBM Bluemix Cloud Object Storage, which supports a dialect of the AWS S3 API.
Most functionality provided by the Alpakka S3 connector is compatible with Cloud Object Store, but there are a few limitations, which are listed below.

## Connection limitations

- This S3 connector does not support domain-style access for Cloud Object Store, so only path-style access is supported.
- Regions in COS are always part of the host/endpoint, therefore leave the s3Region field in S3Settings empty
- The object proxy, containing host/endpoint, port and scheme, must always be specified.

## External references

[IBM Cloud Object Storage Documentation](https://ibm-public-cos.github.io/crs-docs/api-reference)

## Example

Scala
: @@snip ($alpakka$/s3/src/test/scala/akka/stream/alpakka/s3/scaladsl/DocSnippets.scala) { #scala-bluemix-example }

Java
: @@snip ($alpakka$/s3/src/test/java/akka/stream/alpakka/s3/javadsl/DocSnippets.java) { #java-bluemix-example }
